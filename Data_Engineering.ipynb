{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 1 -  DATA STRUCTURING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the directory containing the BBC_articles folder\n",
    "directory = 'BBC_articles'\n",
    "\n",
    "# Create an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        # Extract article_id and category from the filename\n",
    "        article_id, category = filename.split('_')\n",
    "        \n",
    "        # Read the text content of the file\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # Append the data to the list\n",
    "        data.append({'article_id': article_id, 'text': text, 'category': category})\n",
    "\n",
    "# Create a DataFrame from the data list\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('bbc_articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('bbc_articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 2 -Data Preprocessing for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenize the text data\n",
    "df['tokenized_text'] = df['text'].apply(word_tokenize)\n",
    "\n",
    "# Define a function to preprocess the tokens\n",
    "def preprocess_tokens(tokens):\n",
    "    preprocessed_tokens = []\n",
    "    for token in tokens:\n",
    "        # Remove numerals\n",
    "        token = re.sub(r'\\d+', '', token)\n",
    "        \n",
    "        # Convert to lowercase and remove punctuation\n",
    "        token = ''.join([c for c in token if c not in string.punctuation])\n",
    "        token = token.lower()\n",
    "        \n",
    "        # Lemmatize\n",
    "        lemmatized_token = lemmatizer.lemmatize(token)\n",
    "        \n",
    "        preprocessed_tokens.append(lemmatized_token)\n",
    "    \n",
    "    return preprocessed_tokens\n",
    "\n",
    "# Apply the preprocessing function to the tokenized text\n",
    "df['preprocessed_text'] = df['tokenized_text'].apply(preprocess_tokens)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['preprocessed_text'] = df['preprocessed_text'].apply(lambda tokens: [token for token in tokens if token not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TF-IDF vectorizer with a maximum of 10,000 features\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "\n",
    "# Fit and transform the preprocessed text data\n",
    "X = vectorizer.fit_transform(df['preprocessed_text'].apply(' '.join))\n",
    "\n",
    "# Create a new DataFrame with the numerical features and labels\n",
    "vectorized_data = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "vectorized_data['category'] = df['category']\n",
    "\n",
    "# Save the vectorized dataset as a new CSV file with limited floating-point precision\n",
    "vectorized_data.to_csv('vectorized_dataset.csv', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Save the vectorized dataset as a compressed CSV file\n",
    "vectorized_data.to_csv('vectorized_dataset.csv.gz', index=False, float_format='%.4f', compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
